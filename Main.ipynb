{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-07T20:04:24.001696Z",
     "start_time": "2025-04-07T20:04:23.994559Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T20:04:24.422854Z",
     "start_time": "2025-04-07T20:04:24.417772Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "5d61d7de1a9f63df",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T20:04:24.861017Z",
     "start_time": "2025-04-07T20:04:24.852625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ],
   "id": "93c77fed839764f8",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T20:04:27.178444Z",
     "start_time": "2025-04-07T20:04:25.598070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=4)"
   ],
   "id": "5b7392884ca20b56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T20:04:27.297937Z",
     "start_time": "2025-04-07T20:04:27.291176Z"
    }
   },
   "cell_type": "code",
   "source": "class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']",
   "id": "99091d768eca54e8",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T20:04:27.434557Z",
     "start_time": "2025-04-07T20:04:27.427985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 4 * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ],
   "id": "1fa6553c939fe340",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T20:36:08.842766Z",
     "start_time": "2025-04-07T20:16:22.980308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = NeuralNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "\n",
    "num_epochs = 40\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1:03d} | Loss: {running_loss/len(train_loader):.4f} | Train Acc: {acc:.2f}% | Time: {time.time()-start:.1f}s\")"
   ],
   "id": "2c567303d04d866e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 1.6883 | Train Acc: 37.75% | Time: 25.1s\n",
      "Epoch 002 | Loss: 1.2310 | Train Acc: 55.63% | Time: 27.4s\n",
      "Epoch 003 | Loss: 1.0446 | Train Acc: 63.06% | Time: 25.3s\n",
      "Epoch 004 | Loss: 0.9013 | Train Acc: 68.59% | Time: 27.4s\n",
      "Epoch 005 | Loss: 0.7976 | Train Acc: 72.61% | Time: 27.2s\n",
      "Epoch 006 | Loss: 0.7248 | Train Acc: 75.67% | Time: 25.6s\n",
      "Epoch 007 | Loss: 0.6554 | Train Acc: 78.05% | Time: 27.7s\n",
      "Epoch 008 | Loss: 0.6007 | Train Acc: 80.09% | Time: 28.9s\n",
      "Epoch 009 | Loss: 0.5615 | Train Acc: 81.41% | Time: 30.5s\n",
      "Epoch 010 | Loss: 0.5192 | Train Acc: 82.88% | Time: 28.9s\n",
      "Epoch 011 | Loss: 0.4817 | Train Acc: 84.19% | Time: 28.5s\n",
      "Epoch 012 | Loss: 0.4473 | Train Acc: 85.30% | Time: 28.8s\n",
      "Epoch 013 | Loss: 0.4285 | Train Acc: 85.70% | Time: 29.3s\n",
      "Epoch 014 | Loss: 0.4008 | Train Acc: 86.82% | Time: 30.8s\n",
      "Epoch 015 | Loss: 0.3759 | Train Acc: 87.61% | Time: 28.5s\n",
      "Epoch 016 | Loss: 0.3629 | Train Acc: 88.05% | Time: 32.6s\n",
      "Epoch 017 | Loss: 0.3411 | Train Acc: 88.86% | Time: 29.1s\n",
      "Epoch 018 | Loss: 0.3186 | Train Acc: 89.37% | Time: 29.4s\n",
      "Epoch 019 | Loss: 0.3074 | Train Acc: 89.88% | Time: 30.4s\n",
      "Epoch 020 | Loss: 0.2929 | Train Acc: 90.32% | Time: 31.2s\n",
      "Epoch 021 | Loss: 0.2784 | Train Acc: 90.90% | Time: 30.2s\n",
      "Epoch 022 | Loss: 0.2593 | Train Acc: 91.47% | Time: 30.6s\n",
      "Epoch 023 | Loss: 0.2532 | Train Acc: 91.58% | Time: 30.7s\n",
      "Epoch 024 | Loss: 0.2412 | Train Acc: 92.15% | Time: 31.7s\n",
      "Epoch 025 | Loss: 0.2263 | Train Acc: 92.42% | Time: 31.0s\n",
      "Epoch 026 | Loss: 0.2168 | Train Acc: 92.72% | Time: 30.2s\n",
      "Epoch 027 | Loss: 0.2103 | Train Acc: 93.08% | Time: 30.9s\n",
      "Epoch 028 | Loss: 0.1977 | Train Acc: 93.53% | Time: 32.2s\n",
      "Epoch 029 | Loss: 0.1894 | Train Acc: 93.80% | Time: 29.1s\n",
      "Epoch 030 | Loss: 0.1822 | Train Acc: 94.02% | Time: 30.5s\n",
      "Epoch 031 | Loss: 0.1350 | Train Acc: 95.63% | Time: 30.4s\n",
      "Epoch 032 | Loss: 0.1197 | Train Acc: 96.13% | Time: 30.0s\n",
      "Epoch 033 | Loss: 0.1127 | Train Acc: 96.23% | Time: 30.1s\n",
      "Epoch 034 | Loss: 0.1049 | Train Acc: 96.55% | Time: 30.2s\n",
      "Epoch 035 | Loss: 0.1002 | Train Acc: 96.65% | Time: 30.0s\n",
      "Epoch 036 | Loss: 0.0977 | Train Acc: 96.75% | Time: 29.2s\n",
      "Epoch 037 | Loss: 0.0918 | Train Acc: 96.85% | Time: 29.9s\n",
      "Epoch 038 | Loss: 0.0938 | Train Acc: 96.90% | Time: 29.7s\n",
      "Epoch 039 | Loss: 0.0859 | Train Acc: 97.17% | Time: 29.5s\n",
      "Epoch 040 | Loss: 0.0838 | Train Acc: 97.38% | Time: 37.1s\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T20:36:09.124194Z",
     "start_time": "2025-04-07T20:36:09.084113Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), 'trained_net.pth')",
   "id": "9c61b72793fde227",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T20:36:37.712152Z",
     "start_time": "2025-04-07T20:36:37.652616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = NeuralNet().to(device)\n",
    "model.load_state_dict(torch.load('trained_net.pth'))"
   ],
   "id": "c0e662d92f32059d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohib\\AppData\\Local\\Temp\\ipykernel_51452\\2857926675.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('trained_net.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T20:36:36.301639Z",
     "start_time": "2025-04-07T20:36:09.401450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ],
   "id": "5351a8299e68ac0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 90.97%\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T20:36:37.525336Z",
     "start_time": "2025-04-07T20:36:37.079198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = new_transform(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "    return image\n",
    "\n",
    "\n",
    "image_paths = ['images/airplane.jpeg', 'images/dog.jpeg']\n",
    "images = [load_image(img) for img in image_paths]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for image in images:\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        print(f'predicted: {class_names[predicted.item()]}')\n"
   ],
   "id": "9e2739aeca780cab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: plane\n",
      "predicted: dog\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9243dd723ee1cedd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "71c54d493587322f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
